{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from models.layers.layers import trunc_normal_, to_2tuple\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(\"cuda:1\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 256\n",
    "W= 256\n",
    "\n",
    "transform = T.Compose([\n",
    "T.Resize((H,W)),\n",
    "T.ToTensor(),\n",
    "T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(dim, eps=1e-5, momentum=0.1, track_running_stats=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, head_dim_ratio=1., qkv_bias=False, qk_scale=None,\n",
    "                 attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = round(dim // num_heads * head_dim_ratio)\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.qkv(x)\n",
    "        qkv = rearrange(x, 'b (x y z) h w -> x b y (h w) z', x=3, y=self.num_heads, z=self.head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "        x = rearrange(x, 'b y (h w) z -> b (y z) h w', h=H, w=W)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, head_dim_ratio=1., mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=LayerNorm,\n",
    "                 group=8, attn_disabled=False, spatial_conv=False):\n",
    "        super().__init__()\n",
    "        self.attn_disabled = attn_disabled\n",
    "        self.spatial_conv = spatial_conv\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        if not attn_disabled:\n",
    "            self.norm1 = norm_layer(dim)\n",
    "            self.attn = Attention(dim, num_heads=num_heads, head_dim_ratio=head_dim_ratio, qkv_bias=qkv_bias,\n",
    "                                  qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,\n",
    "                       group=group, spatial_conv=spatial_conv) # new setting\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.attn_disabled:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0., group=8, spatial_conv=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.spatial_conv = spatial_conv\n",
    "        if self.spatial_conv:\n",
    "            if group < 2: #net setting\n",
    "                hidden_features = in_features * 5 // 6\n",
    "            else:\n",
    "                hidden_features = in_features * 2\n",
    "        self.hidden_features = hidden_features\n",
    "        self.group = group\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0, bias=False)\n",
    "        self.act1 = act_layer()\n",
    "        if self.spatial_conv:\n",
    "            self.conv2 = nn.Conv2d(hidden_features, hidden_features, 3, stride=1, padding=1,\n",
    "                                   groups=self.group, bias=False)\n",
    "            self.act2 = act_layer()\n",
    "        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        if self.spatial_conv:\n",
    "            x = self.conv2(x)\n",
    "            x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm_pe = norm_layer is not None\n",
    "        if self.norm_pe:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) does not match model ({self.img_size[1]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        if self.norm_pe:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visformer(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=16, init_channels=32, num_classes=1000, embed_dim=384, depth=12,\n",
    "                 num_heads=6, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=LayerNorm, attn_stage='111', pos_embed=True, spatial_conv='111',\n",
    "                 vit_embedding=False, group=8, pool=True, conv_init=False, embedding_norm=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.init_channels = init_channels\n",
    "        self.img_size = img_size\n",
    "        self.vit_embedding = vit_embedding\n",
    "        self.pool = pool\n",
    "        self.conv_init = conv_init\n",
    "        if isinstance(depth, list) or isinstance(depth, tuple):\n",
    "            self.stage_num1, self.stage_num2, self.stage_num3 = depth\n",
    "            depth = sum(depth)\n",
    "        else:\n",
    "            self.stage_num1 = self.stage_num3 = depth // 3\n",
    "            self.stage_num2 = depth - self.stage_num1 - self.stage_num3\n",
    "        self.pos_embed = pos_embed\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "\n",
    "        # stage 1\n",
    "        if self.vit_embedding:\n",
    "            self.using_stem = False\n",
    "            self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=16, in_chans=3, embed_dim=embed_dim,\n",
    "                                           norm_layer=embedding_norm)\n",
    "            img_size //= 16\n",
    "        else:\n",
    "            if self.init_channels is None:\n",
    "                self.using_stem = False\n",
    "                self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=8, in_chans=3, embed_dim=embed_dim//2,\n",
    "                                               norm_layer=embedding_norm)\n",
    "                img_size //= 8\n",
    "            else:\n",
    "                self.using_stem = True\n",
    "                self.stem = nn.Sequential(\n",
    "                    nn.Conv2d(3, self.init_channels, 7, stride=2, padding=3, bias=False),\n",
    "                    BatchNorm(self.init_channels),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                img_size //= 2\n",
    "                self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=4, in_chans=self.init_channels,\n",
    "                                               embed_dim=embed_dim//2, norm_layer=embedding_norm)\n",
    "                img_size //= 4\n",
    "\n",
    "        if self.pos_embed:\n",
    "            if self.vit_embedding:\n",
    "                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim, img_size, img_size))\n",
    "            else:\n",
    "                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim//2, img_size, img_size))\n",
    "            self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.stage1 = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim//2, num_heads=num_heads, head_dim_ratio=0.5, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                group=group, attn_disabled=(attn_stage[0] == '0'), spatial_conv=(spatial_conv[0] == '1')\n",
    "            )\n",
    "            for i in range(self.stage_num1)\n",
    "        ])\n",
    "\n",
    "        #stage2\n",
    "        if not self.vit_embedding:\n",
    "            self.patch_embed2 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim//2, embed_dim=embed_dim,\n",
    "                                           norm_layer=embedding_norm)\n",
    "            img_size //= 2\n",
    "            if self.pos_embed:\n",
    "                self.pos_embed2 = nn.Parameter(torch.zeros(1, embed_dim, img_size, img_size))\n",
    "        self.stage2 = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, head_dim_ratio=1.0, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                group=group, attn_disabled=(attn_stage[1] == '0'), spatial_conv=(spatial_conv[1] == '1')\n",
    "            )\n",
    "            for i in range(self.stage_num1, self.stage_num1+self.stage_num2)\n",
    "        ])\n",
    "\n",
    "        # stage 3\n",
    "        if not self.vit_embedding:\n",
    "            self.patch_embed3 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim, embed_dim=embed_dim*2,\n",
    "                                           norm_layer=embedding_norm)\n",
    "            img_size //= 2\n",
    "            if self.pos_embed:\n",
    "                self.pos_embed3 = nn.Parameter(torch.zeros(1, embed_dim*2, img_size, img_size))\n",
    "        self.stage3 = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim*2, num_heads=num_heads, head_dim_ratio=1.0, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                group=group, attn_disabled=(attn_stage[2] == '0'), spatial_conv=(spatial_conv[2] == '1')\n",
    "            )\n",
    "            for i in range(self.stage_num1+self.stage_num2, depth)\n",
    "        ])\n",
    "\n",
    "        # head\n",
    "        if self.pool:\n",
    "            self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        if not self.vit_embedding:\n",
    "            self.norm = norm_layer(embed_dim*2)\n",
    "            self.head = nn.Linear(embed_dim*2, num_classes)\n",
    "        else:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "            self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # weights init\n",
    "        if self.pos_embed:\n",
    "            trunc_normal_(self.pos_embed1, std=0.02)\n",
    "            if not self.vit_embedding:\n",
    "                trunc_normal_(self.pos_embed2, std=0.02)\n",
    "                trunc_normal_(self.pos_embed3, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            if self.conv_init:\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            else:\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.using_stem:\n",
    "            x = self.stem(x)\n",
    "\n",
    "        # stage 1\n",
    "        x = self.patch_embed1(x)\n",
    "        if self.pos_embed:\n",
    "            x = x + self.pos_embed1\n",
    "            x = self.pos_drop(x)\n",
    "        for b in self.stage1:\n",
    "            x = b(x)\n",
    "\n",
    "        # stage 2\n",
    "        if not self.vit_embedding:\n",
    "            x = self.patch_embed2(x)\n",
    "            if self.pos_embed:\n",
    "                x = x + self.pos_embed2\n",
    "                x = self.pos_drop(x)\n",
    "        for b in self.stage2:\n",
    "            x = b(x)\n",
    "\n",
    "        # stage3\n",
    "        if not self.vit_embedding:\n",
    "            x = self.patch_embed3(x)\n",
    "            if self.pos_embed:\n",
    "                x = x + self.pos_embed3\n",
    "                x = self.pos_drop(x)\n",
    "        for b in self.stage3:\n",
    "            x = b(x)\n",
    "\n",
    "        # head\n",
    "        x = self.norm(x)\n",
    "        print(x.shape)\n",
    "        if self.pool:\n",
    "            x = self.global_pooling(x)\n",
    "            print(\"after pool\",x.shape)\n",
    "        else:\n",
    "            x = x[:, :, 0, 0]\n",
    "        print(x.shape)\n",
    "        x = self.head( x.view(x.size(0), -1) )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net2(**kwargs):\n",
    "    model = Visformer(init_channels=32, embed_dim=384, depth=[0,12,0], num_heads=6, mlp_ratio=4., attn_stage='111',\n",
    "                      spatial_conv='000', vit_embedding=False, norm_layer=LayerNorm, conv_init=True, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net2()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://southernboating.com/wp-content/uploads/2019/11/New-running-530-0832-1024x600.jpg'\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "img = transform(im).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 8, 8])\n",
      "after pool torch.Size([1, 768, 1, 1])\n",
      "torch.Size([1, 768, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((384,256))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = nn.AdaptiveAvgPool2d(1)\n",
    "gp(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
