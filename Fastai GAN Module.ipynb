{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.distributed import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.metrics import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "from fastai import torch_core\n",
    "\n",
    "\n",
    "from fastprogress import fastprogress\n",
    "import torch\n",
    "import argparse\n",
    "#from models.utils.gan_joiner import GAN\n",
    "from models.utils.joiner3 import *\n",
    "from models.utils.new_losses import *\n",
    "from models.utils.metrics import *\n",
    "from models.utils.misc import *\n",
    "from models.unet import UNet\n",
    "from models.utils.datasets import *\n",
    "from models.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "H = 256\n",
    "W= 256\n",
    "bs =5\n",
    "grid_l = 16\n",
    "nclass = 10\n",
    "pf = \"3\"\n",
    "epochs = 1\n",
    "\n",
    "beta = 0.00001 #0.0002\n",
    "beta_str = '5e-6'\n",
    "gamma = 0.0005\n",
    "sigma = 1.0\n",
    "\n",
    "lr = 3e-4\n",
    "lr_str = '3e-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "path = Path.home()/'Luiz/gan_attention/data/Custom_ImageNet'\n",
    "save_path = 'data/Custom_ImageNet'\n",
    "\n",
    "def get_gm(r):\n",
    "    label = parent_label(r)\n",
    "    a = attrgetter(\"name\")\n",
    "    rgex = RegexLabeller(pat = r'image(.*?).jpeg') \n",
    "    gm = torch.load(save_path+\"/gramm/\"+str(label)+\"/gm\"+rgex(a(r))+\".pt\")\n",
    "    return gm, TensorCategory(int(label))\n",
    "\n",
    "transform = ([*aug_transforms(),Normalize.from_stats([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "dblock = DataBlock(blocks    = (ImageBlock, [RegressionBlock, CategoryBlock]),\n",
    "                   n_inp=1,\n",
    "                   get_items = get_image_files,\n",
    "                   #get_y     = parent_label,\n",
    "                   get_y     = get_gm,                   \n",
    "                   splitter  = RandomSplitter(0.99),\n",
    "                   item_tfms = Resize(256),\n",
    "                   #batch_tfms= Normalize.from_stats(*imagenet_stats)\n",
    "                   batch_tfms= transform\n",
    "                  )\n",
    "dsets = dblock.datasets(path/\"images\")\n",
    "\n",
    "dloader = dblock.dataloaders(path/\"images\", bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dloader.train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models and Losses\n",
    "gen = UNet(n_channels=3, n_classes=3, bilinear=False)\n",
    "crt = ImageNetJoiner(num_encoder_layers = 6, nhead=8, num_classes = nclass, batch_size=bs, hidden_dim=512, image_h=H, image_w=W, grid_l=grid_l)\n",
    "\n",
    "generator_loss = GeneratorLoss(beta, gamma,sigma)\n",
    "critic_loss = CriticLoss(beta=beta, sigma=sigma, enc_layer_idx=0, pf=pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_freeze_model(m, rg):\n",
    "    if type(m) == ImageNetJoiner:\n",
    "        m.paramsToUpdate()\n",
    "    else:\n",
    "        for p in m.parameters(): p.requires_grad_(rg)\n",
    "set_freeze_model = _set_freeze_model\n",
    "\n",
    "class _GANModule(Module):\n",
    "    \"Wrapper around a `generator` and a `critic` to create a GAN.\"\n",
    "    def __init__(self, generator=None, critic=None, gen_mode=False):\n",
    "        if generator is not None: self.generator=generator\n",
    "        if critic    is not None: self.critic   =critic\n",
    "        store_attr('gen_mode')\n",
    "\n",
    "    def forward(self, *args):\n",
    "        #print(self.gen_mode)\n",
    "        return self.generator(*args) if self.gen_mode else self.critic(*args)\n",
    "\n",
    "    def switch(self, gen_mode=None):\n",
    "        \"Put the module in generator mode if `gen_mode`, in critic mode otherwise.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n",
    "GANModule = _GANModule\n",
    "\n",
    "class _GANLoss(GANModule):\n",
    "    \"Wrapper around `crit_loss_func` and `gen_loss_func`\"\n",
    "    def __init__(self, gen_loss_func, crit_loss_func, gan_model):\n",
    "        super().__init__()\n",
    "        store_attr('gen_loss_func,crit_loss_func,gan_model')\n",
    "\n",
    "    def generator(self, input, output, target):\n",
    "        \"Evaluate the `output` with the critic then uses `self.gen_loss_func`\"\n",
    "        fake_pred = self.gan_model.critic(output[0])\n",
    "        self.gen_loss = self.gen_loss_func(fake_pred, output, target)\n",
    "        return self.gen_loss\n",
    "\n",
    "    def critic(self, input, real_pred, target):\n",
    "        \"Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.crit_loss_func`.\"\n",
    "        #print(input.shape)\n",
    "        with torch.no_grad():\n",
    "            fake = self.gan_model.generator(input)#.requires_grad_(False)\n",
    "        fake_pred = self.gan_model.critic(fake[0])\n",
    "        self.crit_loss = self.crit_loss_func(real_pred, target) + self.crit_loss_func(fake_pred, target)\n",
    "        return self.crit_loss\n",
    "\n",
    "GANLoss = _GANLoss\n",
    "\n",
    "@delegates()\n",
    "class _GANLearner(Learner):\n",
    "    \"A `Learner` suitable for GANs.\"\n",
    "    def __init__(self, dls, generator, critic, gen_loss_func, crit_loss_func, switcher=None, gen_first=False,\n",
    "                 switch_eval=True, show_img=True, clip=None, cbs=None, metrics=None, **kwargs):\n",
    "        #print(\"Creating Learner\")\n",
    "        gan = GANModule(generator, critic)\n",
    "        loss_func = GANLoss(gen_loss_func, crit_loss_func, gan)\n",
    "        if switcher is None: switcher = FixedGANSwitcher(n_crit=5, n_gen=1)\n",
    "        trainer = GANTrainer(clip=clip, switch_eval=switch_eval, gen_first=gen_first, show_img=show_img)\n",
    "        cbs = L(cbs) + L(trainer, switcher)\n",
    "        metrics = L(metrics) + L(*LossMetrics('gen_loss,crit_loss'))\n",
    "        super().__init__(dls, gan, loss_func=loss_func, cbs=cbs, metrics=metrics, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_learners(cls, gen_learn, crit_learn, switcher=None, weights_gen=None, **kwargs):\n",
    "        \"Create a GAN from `learn_gen` and `learn_crit`.\"\n",
    "        losses = gan_loss_from_func(gen_learn.loss_func, crit_learn.loss_func, weights_gen=weights_gen)\n",
    "        return cls(gen_learn.dls, gen_learn.model, crit_learn.model, *losses, switcher=switcher, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def wgan(cls, dls, generator, critic, switcher=None, clip=0.01, switch_eval=False, **kwargs):\n",
    "        \"Create a WGAN from `data`, `generator` and `critic`.\"\n",
    "        return cls(dls, generator, critic, _tk_mean, _tk_diff, switcher=switcher, clip=clip, switch_eval=switch_eval, **kwargs)\n",
    "\n",
    "GANLearner = _GANLearner\n",
    "\n",
    "\n",
    "def _before_batch(self):\n",
    "    \"Clamp the weights with `self.clip` if it's not None, set the correct input/target.\"\n",
    "    if self.training and self.clip is not None:\n",
    "        for p in self.critic.parameters(): p.data.clamp_(-self.clip, self.clip)\n",
    "    if not self.gen_mode:\n",
    "        (self.learn.xb,self.learn.yb) = (self.xb,self.yb)\n",
    "GANTrainer.before_batch = _before_batch\n",
    "\n",
    "\n",
    "def __set_trainable(self):\n",
    "    train_model = self.generator if     self.gen_mode else self.critic\n",
    "    loss_model  = self.generator if not self.gen_mode else self.critic\n",
    "    set_freeze_model(train_model, True)\n",
    "    set_freeze_model(loss_model, False)\n",
    "    if self.switch_eval:\n",
    "        train_model.train()\n",
    "        loss_model.eval()\n",
    "GANTrainer._set_trainable = __set_trainable\n",
    "\n",
    "\n",
    "def __do_one_batch(self):\n",
    "    #print(\"type x\", type(*self.xb))\n",
    "    #print(\"type y\", type(*self.yb))\n",
    "    self.pred = self.model(*self.xb)\n",
    "    self('after_pred')\n",
    "    if len(self.yb):\n",
    "        self.loss_grad = self.loss_func(*self.xb, self.pred, *self.yb)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "    self('after_loss')\n",
    "    if not self.training or not len(self.yb): return\n",
    "    self('before_backward')\n",
    "    self.loss_grad.backward()\n",
    "    self._with_events(self.opt.step, 'step', CancelStepException)\n",
    "    self.opt.zero_grad()\n",
    "Learner._do_one_batch = __do_one_batch\n",
    "\n",
    "###########################\n",
    "############################\n",
    "\n",
    "class _FixedGANSwitcher(Callback):\n",
    "    \"Switcher to do `n_crit` iterations of the critic then `n_gen` iterations of the generator.\"\n",
    "    run_after = GANTrainer\n",
    "    def __init__(self, n_crit=1, n_gen=1): store_attr('n_crit,n_gen')\n",
    "    def before_train(self): self.n_c,self.n_g = 0,0\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Switch the model if necessary.\"\n",
    "        #print(\"After Batch\")\n",
    "        if not self.training: return\n",
    "        if self.learn.gan_trainer.gen_mode:\n",
    "            self.n_g += 1\n",
    "            n_iter,n_in,n_out = self.n_gen,self.n_c,self.n_g\n",
    "        else:\n",
    "            #print(\"After batch Else\")\n",
    "            self.n_c += 1\n",
    "            n_iter,n_in,n_out = self.n_crit,self.n_g,self.n_c\n",
    "        target = n_iter if isinstance(n_iter, int) else n_iter(n_in)\n",
    "        #print(target)\n",
    "        #print(n_out)\n",
    "        if target == n_out:\n",
    "            self.learn.gan_trainer.switch()\n",
    "            self.n_c,self.n_g = 0,0\n",
    "FixedGANSwitcher = _FixedGANSwitcher\n",
    "\n",
    "def _switch(self, gen_mode=None):\n",
    "    \"Switch the model and loss function, if `gen_mode` is provided, in the desired mode.\"\n",
    "    self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n",
    "    self._set_trainable()\n",
    "    #self.model.switch(gen_mode)\n",
    "    #self.loss_func.switch(gen_mode)\n",
    "GANTrainer.switch = _switch\n",
    "\n",
    "def __set_trainable(self):\n",
    "    train_model = self.generator if     self.gen_mode else self.critic\n",
    "    loss_model  = self.generator if not self.gen_mode else self.critic\n",
    "    set_freeze_model(train_model, True)\n",
    "    set_freeze_model(loss_model, False)\n",
    "    if self.switch_eval:\n",
    "        train_model.train()\n",
    "        loss_model.eval()\n",
    "GANTrainer._set_trainable = __set_trainable\n",
    "\n",
    "def _accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        if self.attr == 'gen_loss':\n",
    "            self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        else:\n",
    "            self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        self.count += bs\n",
    "LossMetric.accumulate = _accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Accuracy(preds,target): \n",
    "    if len(preds) == 2:\n",
    "        fakePreds = learner.gan_trainer.critic(preds[0])\n",
    "        _, pred = torch.max(fakePreds[0], 1)\n",
    "        return (pred.cuda() == target[1].cuda()).float().mean()\n",
    "    else:\n",
    "        _, pred = torch.max(preds[0], 1)\n",
    "        return (pred.cuda() == target[1].cuda()).float().mean()\n",
    "    \n",
    "MSE = nn.MSELoss()    \n",
    "def GAN_Reconstruction_Loss(preds,target,sigma=1):\n",
    "    if len(preds) == 2:\n",
    "        Lrec = sigma*MSE(preds[0],preds[1])\n",
    "    else:\n",
    "        Lrec = 0.000\n",
    "  \n",
    "    return Lrec\n",
    "\n",
    "LCA2 = Curating_of_attention_loss(pf=\"2\")\n",
    "def GAN_Curating_Of_Attention_Loss(preds,target):\n",
    "    if len(preds) == 2:\n",
    "        fakePreds = learner.gan_trainer.critic(preds[0])\n",
    "        Latt = LCA2(fakePreds[1], fakePreds[3])\n",
    "        return (beta*Latt).float().mean()\n",
    "    else:\n",
    "        Latt = LCA2(preds[1], preds[3])\n",
    "    \n",
    "        return (beta*Latt).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = GANLearner(dloader,gen,crt,generator_loss,critic_loss,gen_first=True,metrics=[GAN_Accuracy,GAN_Reconstruction_Loss,GAN_Curating_Of_Attention_Loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
