{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils.fastai.basics import *\n",
    "from models.utils.fastai.vision.all import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModule(Module):\n",
    "    \"Wrapper around a `generator` and a `critic` to create a GAN.\"\n",
    "    def __init__(self, generator=None, critic=None, gen_mode=False):\n",
    "        if generator is not None: self.generator=generator\n",
    "        if critic    is not None: self.critic   =critic\n",
    "        store_attr('gen_mode')\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.generator(*args) if self.gen_mode else self.critic(*args)\n",
    "\n",
    "    def switch(self, gen_mode=None):\n",
    "        \"Put the module in generator mode if `gen_mode`, in critic mode otherwise.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GANModule.switch\" class=\"doc_header\"><code>GANModule.switch</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GANModule.switch</code>(**`gen_mode`**=*`None`*)\n",
       "\n",
       "Put the module in generator mode if `gen_mode`, in critic mode otherwise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GANModule.switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(GANModule):\n",
    "    \"Wrapper around `crit_loss_func` and `gen_loss_func`\"\n",
    "    def __init__(self, gen_loss_func, crit_loss_func, gan_model):\n",
    "        super().__init__()\n",
    "        store_attr('gen_loss_func,crit_loss_func,gan_model')\n",
    "\n",
    "    def generator(self, input, output, target):\n",
    "        \"Evaluate the `output` with the critic then uses `self.gen_loss_func`\"\n",
    "        fake_pred = self.gan_model.critic(output)\n",
    "        self.gen_loss = self.gen_loss_func(fake_pred, output, target)\n",
    "        return self.gen_loss\n",
    "\n",
    "    def critic(self, input, real_pred, target):\n",
    "        \"Create some `fake_pred` with the generator from `input` and compare them to `real_pred` in `self.crit_loss_func`.\"\n",
    "        #print(len(real_pred))\n",
    "        #print(target.shape)\n",
    "        self.gan_model.generator.paramsToUpdate(False)\n",
    "        fake = self.gan_model.generator(input)\n",
    "        fake_pred = self.gan_model.critic(fake)\n",
    "        self.crit_loss = (self.crit_loss_func(real_pred, target) + self.crit_loss_func(fake_pred, target)) / 2\n",
    "        return self.crit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveLoss(Module):\n",
    "    \"Expand the `target` to match the `output` size before applying `crit`.\"\n",
    "    def __init__(self, crit): self.crit = crit\n",
    "    def forward(self, output, target):\n",
    "        return self.crit(output, target[:,None].expand_as(output).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy_thresh_expand(y_pred, y_true, thresh=0.5, sigmoid=True):\n",
    "    \"Compute accuracy after expanding `y_true` to the size of `y_pred`.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred>thresh).byte()==y_true[:,None].expand_as(y_pred).byte()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def set_freeze_model(m, rg):\n",
    "#    for p in m.parameters(): p.requires_grad_(rg)\n",
    "        \n",
    "def set_freeze_model(m, rg):\n",
    "    m.paramsToUpdate(rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANTrainer(Callback):\n",
    "    \"Handles GAN Training.\"\n",
    "    run_after = TrainEvalCallback\n",
    "    def __init__(self, switch_eval=False, clip=None, beta=0.98, gen_first=False, show_img=True):\n",
    "        store_attr('switch_eval,clip,gen_first,show_img')\n",
    "        self.gen_loss,self.crit_loss = AvgSmoothLoss(beta=beta),AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def _set_trainable(self):\n",
    "        train_model = self.generator if     self.gen_mode else self.critic\n",
    "        loss_model  = self.generator if not self.gen_mode else self.critic\n",
    "        set_freeze_model(train_model, True)\n",
    "        set_freeze_model(loss_model, False)\n",
    "        if self.switch_eval:\n",
    "            train_model.train()\n",
    "            loss_model.eval()\n",
    "            \n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Initialize smootheners.\"\n",
    "        self.generator,self.critic = self.model.generator,self.model.critic\n",
    "        self.gen_mode = self.gen_first\n",
    "        self.switch(self.gen_mode)\n",
    "        self.crit_losses,self.gen_losses = [],[]\n",
    "        self.gen_loss.reset() ; self.crit_loss.reset()\n",
    "        #self.recorder.no_val=True\n",
    "        #self.recorder.add_metric_names(['gen_loss', 'disc_loss'])\n",
    "        #self.imgs,self.titles = [],[]\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Switch in generator mode for showing results.\"\n",
    "        self.switch(gen_mode=True)\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Clamp the weights with `self.clip` if it's not None, set the correct input/target.\"\n",
    "        if self.training and self.clip is not None:\n",
    "            for p in self.critic.parameters(): p.data.clamp_(-self.clip, self.clip)\n",
    "        if not self.gen_mode:\n",
    "            (self.learn.xb,self.learn.yb) = (self.xb,self.yb)\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Record `last_loss` in the proper list.\"\n",
    "        if not self.training: return\n",
    "        if self.gen_mode:\n",
    "            self.gen_loss.accumulate(self.learn)\n",
    "            self.gen_losses.append(self.gen_loss.value)\n",
    "            self.last_gen = to_detach(self.pred)\n",
    "        else:\n",
    "            self.crit_loss.accumulate(self.learn)\n",
    "            self.crit_losses.append(self.crit_loss.value)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        \"Put the critic or the generator back to eval if necessary.\"\n",
    "        self.switch(self.gen_mode)\n",
    "\n",
    "    #def after_epoch(self):\n",
    "    #    \"Show a sample image.\"\n",
    "    #    if not hasattr(self, 'last_gen') or not self.show_img: return\n",
    "    #    data = self.learn.data\n",
    "    #    img = self.last_gen[0]\n",
    "    #    norm = getattr(data,'norm',False)\n",
    "    #    if norm and norm.keywords.get('do_y',False): img = data.denorm(img)\n",
    "    #    img = data.train_ds.y.reconstruct(img)\n",
    "    #    self.imgs.append(img)\n",
    "    #    self.titles.append(f'Epoch {epoch}')\n",
    "    #    pbar.show_imgs(self.imgs, self.titles)\n",
    "    #    return add_metrics(last_metrics, [getattr(self.smoothenerG,'smooth',None),getattr(self.smoothenerC,'smooth',None)])\n",
    "\n",
    "    def switch(self, gen_mode=None):\n",
    "        \"Switch the model and loss function, if `gen_mode` is provided, in the desired mode.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n",
    "        self._set_trainable()\n",
    "        self.model.switch(gen_mode)\n",
    "        self.loss_func.switch(gen_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FixedGANSwitcher(Callback):\n",
    "    \"Switcher to do `n_crit` iterations of the critic then `n_gen` iterations of the generator.\"\n",
    "    run_after = GANTrainer\n",
    "    def __init__(self, n_crit=1, n_gen=1): store_attr('n_crit,n_gen')\n",
    "    def before_train(self): self.n_c,self.n_g = 0,0\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Switch the model if necessary.\"\n",
    "        if not self.training: return\n",
    "        if self.learn.gan_trainer.gen_mode:\n",
    "            self.n_g += 1\n",
    "            n_iter,n_in,n_out = self.n_gen,self.n_c,self.n_g\n",
    "        else:\n",
    "            self.n_c += 1\n",
    "            n_iter,n_in,n_out = self.n_crit,self.n_g,self.n_c\n",
    "        target = n_iter if isinstance(n_iter, int) else n_iter(n_in)\n",
    "        if target == n_out:\n",
    "            self.learn.gan_trainer.switch()\n",
    "            self.n_c,self.n_g = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveGANSwitcher(Callback):\n",
    "    \"Switcher that goes back to generator/critic when the loss goes below `gen_thresh`/`crit_thresh`.\"\n",
    "    run_after = GANTrainer\n",
    "    def __init__(self, gen_thresh=None, critic_thresh=None):\n",
    "        store_attr('gen_thresh,critic_thresh')\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Switch the model if necessary.\"\n",
    "        if not self.training: return\n",
    "        if self.gan_trainer.gen_mode:\n",
    "            if self.gen_thresh is None or self.loss < self.gen_thresh: self.gan_trainer.switch()\n",
    "        else:\n",
    "            if self.critic_thresh is None or self.loss < self.critic_thresh: self.gan_trainer.switch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminativeLR(Callback):\n",
    "    \"`Callback` that handles multiplying the learning rate by `mult_lr` for the critic.\"\n",
    "    run_after = GANTrainer\n",
    "    def __init__(self, mult_lr=5.): self.mult_lr = mult_lr\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Multiply the current lr if necessary.\"\n",
    "        if not self.learn.gan_trainer.gen_mode and self.training:\n",
    "            self.learn.opt.set_hyper('lr', self.learn.opt.hypers[0]['lr']*self.mult_lr)\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Put the LR back to its value if necessary.\"\n",
    "        if not self.learn.gan_trainer.gen_mode: self.learn.opt.set_hyper('lr', self.learn.opt.hypers[0]['lr']/self.mult_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss_from_func(loss_gen, loss_crit, weights_gen=None):\n",
    "    \"Define loss functions for a GAN from `loss_gen` and `loss_crit`.\"\n",
    "    def _loss_G(fake_pred, output, target, weights_gen=weights_gen):\n",
    "        ones = fake_pred.new_ones(fake_pred.shape[0])\n",
    "        weights_gen = ifnone(weights_gen, (1.,1.))\n",
    "        return weights_gen[0] * loss_crit(fake_pred, ones) + weights_gen[1] * loss_gen(output, target)\n",
    "\n",
    "    def _loss_C(real_pred, fake_pred):\n",
    "        ones  = real_pred.new_ones (real_pred.shape[0])\n",
    "        zeros = fake_pred.new_zeros(fake_pred.shape[0])\n",
    "        return (loss_crit(real_pred, ones) + loss_crit(fake_pred, zeros)) / 2\n",
    "\n",
    "    return _loss_G, _loss_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tk_mean(fake_pred, output, target): return fake_pred.mean()\n",
    "def _tk_diff(real_pred, fake_pred): return real_pred.mean() - fake_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class GANLearner(Learner):\n",
    "    \"A `Learner` suitable for GANs.\"\n",
    "    def __init__(self, dls, generator, critic, gen_loss_func, crit_loss_func, switcher=None, gen_first=False,\n",
    "                 switch_eval=True, show_img=True, clip=None, cbs=None, metrics=None, **kwargs):\n",
    "        gan = GANModule(generator, critic)\n",
    "        loss_func = GANLoss(gen_loss_func, crit_loss_func, gan)\n",
    "        if switcher is None: switcher = FixedGANSwitcher()\n",
    "        trainer = GANTrainer(clip=clip, switch_eval=switch_eval, gen_first=gen_first, show_img=show_img)\n",
    "        cbs = L(cbs) + L(trainer, switcher)\n",
    "        metrics = L(metrics) + L(*LossMetrics('gen_loss,crit_loss'))\n",
    "        super().__init__(dls, gan, loss_func=loss_func, cbs=cbs, metrics=metrics, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_learners(cls, gen_learn, crit_learn, switcher=None, weights_gen=None, **kwargs):\n",
    "        \"Create a GAN from `learn_gen` and `learn_crit`.\"\n",
    "        losses = gan_loss_from_func(gen_learn.loss_func, crit_learn.loss_func, weights_gen=weights_gen)\n",
    "        return cls(gen_learn.dls, gen_learn.model, crit_learn.model, *losses, switcher=switcher, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def wgan(cls, dls, generator, critic, switcher=None, clip=0.01, switch_eval=False, **kwargs):\n",
    "        \"Create a WGAN from `data`, `generator` and `critic`.\"\n",
    "        if switcher is None: switcher = FixedGANSwitcher(n_crit=5, n_gen=1)\n",
    "        return cls(dls, generator, critic, _tk_mean, _tk_diff, switcher=switcher, clip=clip, switch_eval=switch_eval, **kwargs)\n",
    "\n",
    "GANLearner.from_learners = delegates(to=GANLearner.__init__)(GANLearner.from_learners)\n",
    "GANLearner.wgan = delegates(to=GANLearner.__init__)(GANLearner.wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        print(\"bs\", bs)\n",
    "        if self.attr == 'gen_loss':\n",
    "            self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        else:\n",
    "            self.total += learn.to_detach(getattr(learn.loss_func, self.attr, 0))*bs\n",
    "        self.count += bs\n",
    "#LossMetric.accumulate = _accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Metric():\n",
    "    \"Blueprint for defining a metric\"\n",
    "    def reset(self): pass\n",
    "    def accumulate(self, learn): pass\n",
    "    @property\n",
    "    def value(self): raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def name(self): return class2attr(self, 'Metric')\n",
    "\n",
    "    _docs = dict(\n",
    "        reset=\"Reset inner state to prepare for new computation\",\n",
    "        name=\"Name of the `Metric`, camel-cased and with Metric removed\",\n",
    "        accumulate=\"Use `learn` to update the state with new results\",\n",
    "        value=\"The value of the metric\")\n",
    "Metric = _Metric\n",
    "\n",
    "# Cell\n",
    "def _maybe_reduce(val):\n",
    "    if num_distrib()>1:\n",
    "        val = val.clone()\n",
    "        torch.distributed.all_reduce(val, op=torch.distributed.ReduceOp.SUM)\n",
    "        val /= num_distrib()\n",
    "    return val\n",
    "\n",
    "# Cell\n",
    "class _AvgMetric(_Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func):  self.func = func\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        print(\"AvgMetric\")\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(self.func(*learn.xb, learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__\n",
    "AvgMetric = _AvgMetric\n",
    "# Cell\n",
    "class _AvgLoss(_Metric):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        print(\"AvgLoss\")\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"loss\"\n",
    "AvgLoss = _AvgLoss\n",
    "\n",
    "# Cell\n",
    "class _AvgSmoothLoss(_Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): self.beta = beta\n",
    "    def reset(self):               self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        print(\"AvgSmoothLoss\")\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)\n",
    "AvgSmoothLoss = _AvgSmoothLoss\n",
    "# Cell\n",
    "class ValueMetric(_Metric):\n",
    "    \"Use to include a pre-calculated metric value (for instance calculated in a `Callback`) and returned by `func`\"\n",
    "    def __init__(self, func, metric_name=None): store_attr('func, metric_name')\n",
    "\n",
    "    @property\n",
    "    def value(self): return self.func()\n",
    "\n",
    "    @property\n",
    "    def name(self): return self.metric_name if self.metric_name else self.func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Recorder(Callback):\n",
    "    \"Callback that registers statistics (lr, loss and metrics) during training\"\n",
    "    _stateattrs=('lrs','iters','losses','values')\n",
    "    remove_on_fetch,order = True,50\n",
    "\n",
    "    def __init__(self, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n",
    "        store_attr('add_time,train_metrics,valid_metrics')\n",
    "        self.loss,self.smooth_loss = _AvgLoss(),_AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Prepare state for training\"\n",
    "        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n",
    "        names = self.metrics.attrgot('name')\n",
    "        if self.train_metrics and self.valid_metrics:\n",
    "            names = L('loss') + names\n",
    "            names = names.map('train_{}') + names.map('valid_{}')\n",
    "        elif self.valid_metrics: names = L('train_loss', 'valid_loss') + names\n",
    "        else: names = L('train_loss') + names\n",
    "        if self.add_time: names.append('time')\n",
    "        self.metric_names = 'epoch'+names\n",
    "        self.smooth_loss.reset()\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update all metrics and records lr and smooth loss in training\"\n",
    "        if len(self.yb) == 0: return\n",
    "        mets = self._train_mets if self.training else self._valid_mets\n",
    "        for met in mets: met.accumulate(self.learn)\n",
    "        if not self.training: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.smooth_loss.value)\n",
    "        self.learn.smooth_loss = self.smooth_loss.value\n",
    "\n",
    "    def before_epoch(self):\n",
    "        \"Set timer if `self.add_time=True`\"\n",
    "        self.cancel_train,self.cancel_valid = False,False\n",
    "        if self.add_time: self.start_epoch = time.time()\n",
    "        self.log = L(getattr(self, 'epoch', 0))\n",
    "\n",
    "    def before_train   (self): self._train_mets[1:].map(Self.reset())\n",
    "    def before_validate(self): self._valid_mets.map(Self.reset())\n",
    "    def after_train   (self): self.log += self._train_mets.map(_maybe_item)\n",
    "    def after_validate(self): self.log += self._valid_mets.map(_maybe_item)\n",
    "    def after_cancel_train(self):    self.cancel_train = True\n",
    "    def after_cancel_validate(self): self.cancel_valid = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Store and log the loss/metric values\"\n",
    "        self.learn.final_record = self.log[1:].copy()\n",
    "        self.values.append(self.learn.final_record)\n",
    "        if self.add_time: self.log.append(format_time(time.time() - self.start_epoch))\n",
    "        self.logger(self.log)\n",
    "        self.iters.append(self.smooth_loss.count)\n",
    "\n",
    "    @property\n",
    "    def _train_mets(self):\n",
    "        if getattr(self, 'cancel_train', False): return L()\n",
    "        return L(self.smooth_loss) + (self.metrics if self.train_metrics else L())\n",
    "\n",
    "    @property\n",
    "    def _valid_mets(self):\n",
    "        if getattr(self, 'cancel_valid', False): return L()\n",
    "        return (L(self.loss) + self.metrics if self.valid_metrics else L())\n",
    "\n",
    "    def plot_loss(self, skip_start=5, with_valid=True):\n",
    "        plt.plot(list(range(skip_start, len(self.losses))), self.losses[skip_start:], label='train')\n",
    "        if with_valid:\n",
    "            idx = (np.array(self.iters)<skip_start).sum()\n",
    "            plt.plot(self.iters[idx:], L(self.values[idx:]).itemgot(1), label='valid')\n",
    "            plt.legend()\n",
    "Recorder = _Recorder\n",
    "\n",
    "\n",
    "# def avgmet_accumulate(self, learn):\n",
    "#     print(\"Accumulating\")\n",
    "#     bs = find_bs(learn.yb)\n",
    "#     self.total += learn.to_detach(self.func(*learn.xb, learn.pred, *learn.yb))*bs\n",
    "#     self.count += bs\n",
    "\n",
    "# AvgMetric.accumulate = avgmet_accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Accuracy(input, preds, target): \n",
    "    \n",
    "    \n",
    "    #learner.gan_model.generator.paramsToUpdate(False)\n",
    "    #fake = learner.gan_model.generator(input)\n",
    "    #fake_pred = self.gan_model.critic(fake)\n",
    "    \n",
    "    if len(preds) == 2:\n",
    "        fakePreds = learner.gan_trainer.critic(preds[0])\n",
    "        _, pred = torch.max(fakePreds[0], 1)\n",
    "        return (pred.cuda() == target[1].cuda()).float().mean()\n",
    "    else:\n",
    "        _, pred = torch.max(preds[0], 1)\n",
    "        return (pred.cuda() == target[1].cuda()).float().mean()\n",
    "    \n",
    "def _accuracy(inp, targ, axis=-1):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    pred,targ = flatten_check(inp[0].argmax(dim=axis), targ)\n",
    "    return (pred == targ).float().mean()\n",
    "\n",
    "# Cell\n",
    "def _error_rate(inp, targ, axis=-1):\n",
    "    \"1 - `accuracy`\"\n",
    "    return 1 - accuracy(inp, targ, axis=axis)\n",
    "\n",
    "# Cell\n",
    "def _top_k_accuracy(inp, targ, k=5, axis=-1):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    inp = inp[0].topk(k=k, dim=axis)[1]\n",
    "    targ = targ.unsqueeze(dim=axis).expand_as(inp)\n",
    "    return (inp == targ).sum(dim=-1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils.fastai.metrics import *\n",
    "from models.utils.fastai.callback.tracker import SaveModelCallback\n",
    "from models.utils.fastai import torch_core\n",
    "\n",
    "from models.utils.joiner3 import *\n",
    "from models.utils.misc import *\n",
    "from models.unet import UNet\n",
    "from models.utils.new_losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "H = 256\n",
    "W= 256\n",
    "bs =5\n",
    "grid_l = 16\n",
    "nclass = 10\n",
    "pf = \"3\"\n",
    "epochs = 1\n",
    "\n",
    "beta = 0.00001 #0.0002\n",
    "beta_str = '5e-6'\n",
    "gamma = 0.0005\n",
    "sigma = 1.0\n",
    "\n",
    "lr = 3e-4\n",
    "lr_str = '3e-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = untar_data(URLs.CIFAR)\n",
    "path = untar_data(URLs.IMAGENETTE_320)\n",
    "\n",
    "transform = ([*aug_transforms(),Normalize.from_stats([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "data = DataBlock(blocks=(ImageBlock, CategoryBlock), \n",
    "                 get_items=get_image_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=parent_label,\n",
    "                 item_tfms=Resize(H,W),\n",
    "                 batch_tfms=transform)\n",
    "\n",
    "dloader = data.dataloaders(path,bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = UNet(n_channels=3, n_classes=3, bilinear=False)\n",
    "crt = ImageNetJoiner(num_encoder_layers = 6, nhead=8, num_classes = nclass, batch_size=bs, hidden_dim=512, image_h=H, image_w=W, grid_l=grid_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_loss = GeneratorLoss(beta, gamma,sigma)\n",
    "critic_loss = CriticLoss(beta=beta, sigma=sigma, enc_layer_idx=0, pf=pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_mod = GANModule(gen, crt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_loss = GANLoss(generator_loss,critic_loss,gan_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = GANLearner(dloader,gen,crt,generator_loss,critic_loss,gen_first=True, metrics=[GAN_Accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atsumilab/Luiz/gan_attention/models/utils/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (generator) that exists in the learner. Use `self.learn.generator` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n",
      "/home/atsumilab/Luiz/gan_attention/models/utils/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (critic) that exists in the learner. Use `self.learn.critic` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n",
      "/home/atsumilab/Luiz/gan_attention/models/utils/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (gen_mode) that exists in the learner. Use `self.learn.gen_mode` to avoid this\n",
      "  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>GAN_Accuracy</th>\n",
       "      <th>gen_loss</th>\n",
       "      <th>crit_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>GAN_Accuracy</th>\n",
       "      <th>gen_loss</th>\n",
       "      <th>crit_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>GAN_Accuracy</th>\n",
       "      <th>gen_loss</th>\n",
       "      <th>crit_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.509376</td>\n",
       "      <td>0.884788</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>0.884787</td>\n",
       "      <td>2.543373</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.509376</td>\n",
       "      <td>0.884788</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>0.884787</td>\n",
       "      <td>2.543373</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(1,0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    \"Blueprint for defining a metric\"\n",
    "    def reset(self): pass\n",
    "    def accumulate(self, learn): pass\n",
    "    @property\n",
    "    def value(self): raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def name(self): return class2attr(self, 'Metric')\n",
    "\n",
    "    _docs = dict(\n",
    "        reset=\"Reset inner state to prepare for new computation\",\n",
    "        name=\"Name of the `Metric`, camel-cased and with Metric removed\",\n",
    "        accumulate=\"Use `learn` to update the state with new results\",\n",
    "        value=\"The value of the metric\")\n",
    "\n",
    "# Cell\n",
    "def _maybe_reduce(val):\n",
    "    if num_distrib()>1:\n",
    "        val = val.clone()\n",
    "        torch.distributed.all_reduce(val, op=torch.distributed.ReduceOp.SUM)\n",
    "        val /= num_distrib()\n",
    "    return val\n",
    "\n",
    "# Cell\n",
    "class _AvgMetric(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func):  self.func = func\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(self.func(*learn.xb, learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__\n",
    "\n",
    "# Cell\n",
    "class AvgLoss(Metric):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self):           self.total,self.count = 0.,0\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def value(self): return self.total/self.count if self.count != 0 else None\n",
    "    @property\n",
    "    def name(self):  return \"loss\"\n",
    "\n",
    "# Cell\n",
    "class AvgSmoothLoss(Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): self.beta = beta\n",
    "    def reset(self):               self.count,self.val = 0,tensor(0.)\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)\n",
    "    @property\n",
    "    def value(self): return self.val/(1-self.beta**self.count)\n",
    "\n",
    "# Cell\n",
    "class ValueMetric(Metric):\n",
    "    \"Use to include a pre-calculated metric value (for instance calculated in a `Callback`) and returned by `func`\"\n",
    "    def __init__(self, func, metric_name=None): store_attr('func, metric_name')\n",
    "\n",
    "    @property\n",
    "    def value(self): return self.func()\n",
    "\n",
    "    @property\n",
    "    def name(self): return self.metric_name if self.metric_name else self.func.__name__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
